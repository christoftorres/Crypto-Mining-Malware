
import os
import sys
import csv
import Queue
import threading
import subprocess

from time import sleep

from selenium import webdriver
from selenium.webdriver import Firefox
from selenium.webdriver.firefox.options import Options

from geckoprofiler_controller.control_client import *
from geckoprofiler_controller.control_server import *

exitFlag = 0

def testJSEcoin(driver):
	driver.get("https://jsecoin.com/")

def testCoinHive(driver):
	driver.get("https://coinhive.com/")
	iframes = driver.find_elements_by_tag_name("iframe")
	for iframe in iframes:
		driver.switch_to_frame(iframe)
		driver.find_element_by_id("mining-start").click()

def loadBrowser(options):
	driver = Firefox(executable_path='geckodriver', firefox_options=options)
	driver.implicitly_wait(60)
	installWebExtension(driver, 'modified-geckoprofiler')
	return driver

def installWebExtension(driver, extension):
	driver.execute("INSTALL_ADDON", {'path': os.path.join(os.path.dirname(os.path.abspath(__file__)), extension), 'temporary': True})

def checkIframes(driver):
	iframes = driver.find_elements_by_tag_name("iframe")
	for iframe in iframes:
		if "jsecoin.com" in iframe.get_attribute("src") or "coinhive.com" in iframe.get_attribute("src") or "authedmine.com" in iframe.get_attribute("src"):
			#print(iframe.get_attribute("src"))
			return True

def checkScripts(driver):
	scripts = driver.find_elements_by_tag_name("script")
	for script in scripts:
		if "jsecoin.com" in script.get_attribute("src") or "coinhive.com" in script.get_attribute("src") or "authedmine.com" in script.get_attribute("src"):
			#print(script.get_attribute("src"))
			return True

class crawlerThread(threading.Thread):
   def __init__(self, thread_id, options, queue, control_server):
	   threading.Thread.__init__(self)
	   self.thread_id = thread_id
	   self.options = options
	   self.queue = queue
	   self.control_server = control_server
   def run(self):
	   crawlPage(self.thread_id, self.options, self.queue, self.control_server)

def crawlPage(thread_id, options, queue, control_server):
	while not exitFlag:
		queueLock.acquire()
		if not queue.empty():
			try:
				page = queue.get()
				queueLock.release()
				driver = loadBrowser(options)
				print("[Thread "+str(thread_id)+"] Loading "+page["url"]+"... (Alexa Ranking: "+page["rank"]+")")
				driver.get("http://"+page["url"])
				time.sleep(1)
				profiler_client = ControllerClient(control_server=control_server, save_path=os.path.dirname(os.path.abspath(__file__)), url=driver.current_url)
				profiler_client.connect()
				profiler_client.start_profiler()
				sleep(15) # Wait 15 seconds
				profiler_client.capture_profile(100)
				profiler_client.stop_profiler()
				profiler_client.disconnect()
					#if (checkIframes(driver) or checkScripts(driver)):
						#with open("results.csv", "a") as results:
						#    results.write(page["rank"]+","+page["url"]+",Yes\n")
					#else:
						#with open("results.csv", "a") as results:
						#    results.write(page["rank"]+","+page["url"]+",No\n")
			except:
				print("[Thread "+str(thread_id)+"] Unexpected error: ("+page["url"]+") "+str(sys.exc_info()[0]))
			finally:
				print("[Thread "+str(thread_id)+"] Closing "+page["url"]+"...")
				driver.quit()
			print("[Thread "+str(thread_id)+"] Number of websites in the queue: "+str(queue.qsize()))
		else:
			queueLock.release()

if __name__ == "__main__":
	print("Selenium webdriver Version: %s" % (webdriver.__version__))
	process = subprocess.Popen(["geckodriver", "--version"], stdout=subprocess.PIPE)
	out, err = process.communicate()
	print("Geckodriver Version: %s" % (out.split("\n")[0].split(" ")[1]))

	# Start controller ...
	profiler_server = ServerController()
	profiler_server.start_server()
	#profiler_client = None

	options = Options()
	options.add_argument('-headless')

	queueLock = threading.Lock()
	queue = Queue.Queue()

	# Create threads
	threads = []
	for thread_id in range(2):
		thread = crawlerThread(thread_id, options, queue, profiler_server)
		thread.start()
		threads.append(thread)

    # Fill the queue with websites
	with open('./Alexa/top-2.csv') as csvfile:
		readCSV = csv.reader(csvfile, delimiter=',')
		queueLock.acquire()
		for row in readCSV:
			queue.put({"rank": row[0], "url": row[1]})
		queueLock.release()


	# Wait for queue to empty
	while not queue.empty():
		pass

	# Notify threads it's time to exit
	exitFlag = 1

	# Wait for all threads to complete
	for t in threads:
		t.join()

	# Stop controller ...
	profiler_server.stop_server()
